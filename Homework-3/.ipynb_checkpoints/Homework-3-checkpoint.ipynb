{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework - 3\n",
    "***\n",
    "**Name**: \n",
    "***\n",
    "\n",
    "This assignment is due on Canvas by **5pm on Friday October 19th**. Submit only this Jupyter notebook to Canvas.  Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your classmates and instructors, but **you must write all code and solutions on your own**, and list any people or sources consulted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem - 1 [5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a single Boolean random variable $Y$ (the \"classification\"). Let the prior probability $P(Y= true)$ be $\\pi$. Let's try to find $\\pi$, given a training set $D = (y_1,\\cdots, y_N)$ with $N$ independent samples of $Y$. Furthermore, suppose $p$ of the $N$ samples are positive (i.e, labeled as true) and $n$ of the $N$ samples are negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write down an expression for the likelihood of $D$ (that is, the probability of seeing this particular sequence of examples, given a fixed value of $\\pi$) in terms of $\\pi$, $p$ and $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. By differentiating the log likelihood $L$, find the value of $\\pi$ that maximizes the likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write down the likelihood for the data including the attributes, using the following additional notation:\n",
    "\n",
    "    * $\\alpha_i$ is $P(X_i = true | Y = true)$\n",
    "    * $\\beta_i$ is $P(X_i = true | Y = false)$\n",
    "    * $p_i^+$ is the count of samples for which $X_i = true$ and $Y = true$\n",
    "    * $n_i^+$ is the count of samples for which $X_i = false$ and $Y = true$\n",
    "    * $p_i^-$ is the count of samples for which $X_i = true$ and $Y = false$\n",
    "    * $n_i^-$ is the count of samples for which $X_i = false$ and $Y = false$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. By differentiating the log likelihood L, find the values of $\\alpha_i$ and $\\beta_i$ (in terms of the various counts) that maximize the likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with \"Naive Bayes\" generative model [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T23:00:27.087192Z",
     "start_time": "2018-10-02T23:00:27.083634Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "class SPECT:\n",
    "    def __init__(self):\n",
    "        ff = lambda x, y : loadmat(x)[y]\n",
    "        \n",
    "        self.X_train = ff('data/SPECTtrainData.mat','trainData')\n",
    "        self.y_train = ff('data/SPECTtrainLabels.mat','trainLabels')\n",
    "        \n",
    "        self.X_test = ff('data/SPECTtestData.mat', 'testData')\n",
    "        self.y_test = ff('data/SPECTtestLabels.mat', 'testLabels')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T23:00:27.269981Z",
     "start_time": "2018-10-02T23:00:27.264351Z"
    }
   },
   "outputs": [],
   "source": [
    "# Label normal : 1 abnormal : 0\n",
    "data1 = SPECT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given the `SPECT` class with train and test data. These were created from the medical data on cardiac Single Proton Emission Tomography (SPECT) images of patients and each patient is classified into two categories: normal or abnormal. The database of 267 SPECT images sets (patients) was processed to extract features that summarize the original SPECT images. As a result, you are given a training set of 187 patterns and a test set of 80 patterns, each with 22 binary feature. The goal is to build a generative model of each group (normal: 1 and abnormal: 0), and to use these models to classify future patients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the groups, use the training data to build a probabilistic model, assuming that the different features are independent. The model for a group should have 22 parameters $p_i \\in [0,1]$; the probability of a particular data point $x \\in \\{0, 1\\}^{22}$ is then\n",
    "\n",
    "$$\\prod_{i=1}^{22}p_i^{x_i}(1-p_i)^{1-x_i}$$\n",
    "\n",
    "A natural choice is to set $p_i$ to the proportion of training documents (from that particular group) for which $x_i = 1$. In practice, this can be dangerous - when these are lots of features, and any given feature is 1 only a tiny fraction of the time, there often isn't enough data to reliably estimate all the $p_i$ in this way. Therefore, it is common to smooth the estimates somewhat, by setting:\n",
    "\n",
    "$$ p_i = \\frac{\\text{(number of points with $x_i = 1$)} + n\\tilde{p}}{\\text{(number of points)} + n}$$\n",
    "\n",
    "where n is a small integer and $\\tilde{p}$ is a prior estimate of the value of $p_i$. To keep this simple, use $n=2$ and $\\tilde{p}=0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Now implement Naive Bayes model to classify the test examples. Recall that `NaiveBayes` assumes that the conditional property of the features is independent given the label variable.\n",
    "\n",
    "   * Finish `fit` method to generate probabilistic model for both the groups\n",
    "   * Finish `predict` method to return predicted label for the data point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T23:00:28.130369Z",
     "start_time": "2018-10-02T23:00:28.123508Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class NaiveBayes:\n",
    "    def __init__(self, n = 2, prior = 0.5):\n",
    "        \"\"\"\n",
    "        Create a NaiveBayes classifier\n",
    "        :param n : small integer\n",
    "        :param prior: prior estimate of the value of pi\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n = n\n",
    "        self.prior = prior\n",
    "        self.normal_model = None\n",
    "        self.abnormal_model = None\n",
    "\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Generate probabilistic models for normal and abmornal group.\n",
    "        Use self.normal_model and self.abnormal_model to store \n",
    "        models for normal and abnormal groups respectively\n",
    "        \"\"\"\n",
    "        #TODO: Finish this function\n",
    "        \n",
    "                        \n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Return predicted label for the input example\n",
    "        :param data: input example\n",
    "        \"\"\"\n",
    "            \n",
    "        #TODO: Finish this function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Find the error rate of your Naive Bayes algorithm on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T23:00:29.188080Z",
     "start_time": "2018-10-02T23:00:29.171147Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - [15 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T23:12:05.403704Z",
     "start_time": "2018-10-02T23:12:05.398418Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import pickle, gzip       \n",
    "import numpy as np\n",
    "\n",
    "class Numbers:\n",
    "    \"\"\"\n",
    "    Class to store MNIST data for images of 9 and 8 only\n",
    "    \"\"\" \n",
    "    def __init__(self, location):\n",
    "        # You shouldn't have to modify this class, but you can if you'd like\n",
    "        # Load the dataset\n",
    "        with gzip.open(location, 'rb') as f:\n",
    "            train_set, valid_set, test_set = pickle.load(f)\n",
    " \n",
    "        self.train_x, self.train_y = train_set\n",
    "        train_indices = np.where(self.train_y > 7)\n",
    "        self.train_x, self.train_y = self.train_x[train_indices], self.train_y[train_indices]\n",
    "        self.train_y = self.train_y - 8\n",
    " \n",
    "        self.valid_x, self.valid_y = valid_set\n",
    "        valid_indices = np.where(self.valid_y > 7)\n",
    "        self.valid_x, self.valid_y = self.valid_x[valid_indices], self.valid_y[valid_indices]\n",
    "        self.valid_y = self.valid_y - 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T23:12:06.324098Z",
     "start_time": "2018-10-02T23:12:05.617372Z"
    }
   },
   "outputs": [],
   "source": [
    "data2 = Numbers('data/mnist.pklz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true label: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def view_digit(example, label=None):\n",
    "    if label is not None: print(\"true label: {:d}\".format(label))\n",
    "    plt.imshow(example.reshape(28,28), cmap='gray');\n",
    "#view_digit(data2.train_x[0],data2.train_y[0])\n",
    "view_digit(data2.train_x[1],data2.train_y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you'll implement a Logistic Regression classifier to take drawings of either an eight or a nine and output corresponding label.\n",
    "* Finish the `sigmoid` function to return the output of applying the sigmoid function the input parameter\n",
    "\n",
    "* Finish the `sgd_update` function so that it performs stochastic gradient descent on the single training example and updates the weight vector correspondingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T23:11:28.318182Z",
     "start_time": "2018-10-02T23:11:28.309993Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class LogReg:\n",
    "    \n",
    "    def __init__(self, num_features, eta):\n",
    "        \"\"\"\n",
    "        Create a logistic regression classifier\n",
    "        :param num_features: The number of features (including bias)\n",
    "        :param eta: Learning rate (the default is a constant value)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.w = np.zeros(num_features)\n",
    "        self.eta = eta\n",
    "        \n",
    "        \n",
    "    def sgd_update(self, x_i, y):\n",
    "        \"\"\"\n",
    "        Compute a stochastic gradient update to improve the log likelihood.\n",
    "        :param x_i: The features of the example to take the gradient with respect to\n",
    "        :param y: The target output of the example to take the gradient with respect to\n",
    "        :return: Return the new value of the regression coefficients\n",
    "        \"\"\"\n",
    " \n",
    "        # TODO: Finish this function to do a single stochastic gradient descent update\n",
    "        # and return the updated weight vector\n",
    "        \n",
    "        return self.w\n",
    "    \n",
    "    def sigmoid(self, score, threshold = 20.0):\n",
    "        \"\"\"\n",
    "        Prevent overflow of exp by capping activation at 20.\n",
    "        :param score: A real valued number to convert into a number between 0 and 1\n",
    "        \"\"\"\n",
    "        \n",
    "        if abs(score) > threshold:\n",
    "            score = threshold * np.sign(score)\n",
    "            \n",
    "        # TODO: Finish this function to return the output of applying the sigmoid\n",
    "        # function to the input score (Please do not use external libraries)\n",
    "        \n",
    "        return 1.0 \n",
    "    \n",
    "    def progress(self, examples_x, examples_y):\n",
    "        \"\"\"\n",
    "        Given a set of examples, computes the probability and accuracy\n",
    "        :param examples: The dataset to score\n",
    "        :return: A tuple of (log probability, accuracy)\n",
    "        \"\"\"\n",
    " \n",
    "        logprob = 0.0\n",
    "        num_right = 0\n",
    "        for x_i, y in zip(examples_x, examples_y):\n",
    "            p = self.sigmoid(self.w.dot(x_i))\n",
    "            if y == 1:\n",
    "                logprob += np.log(p)\n",
    "            else:\n",
    "                logprob += np.log(1.0 - p)\n",
    " \n",
    "            # Get accuracy\n",
    "            if abs(y - p) < 0.5:\n",
    "                num_right += 1\n",
    " \n",
    "        return logprob, float(num_right) / float(len(examples_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** After completing the class above, loop over the training data and perform stochastic gradient descent for three different user-defined number of epochs, and five different values of eta range [1e-3, 1]. Train your model and do the following:\n",
    "\n",
    "* Using the `progress` method, calculate the accuracy on the training and the valid sets every 100 iterations. Plot them on same graph for every comparison.\n",
    "\n",
    "* Using `progress` method, calculate the accuracy on the validation set and store it for every epoch.\n",
    "\n",
    "Don't forget to shuffle your training data after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-03T00:26:04.749659Z",
     "start_time": "2018-10-03T00:26:04.746391Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loop over training data and perform updates\n",
    "# Sample code:\n",
    "# lr = LogReg(data2.train_x.shape[1], eta)\n",
    "# iteration = 0\n",
    "# for epoch in range(epochs):\n",
    "# shuffle the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Describe the role of learning rate (eta) on the efficiency of convergence during training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Describe the role of the number of epochs on validation accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
